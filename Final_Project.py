'''
Initial housekeeping
'''

import os
import pandas as pd
import us

import requests
from bs4 import BeautifulSoup
import tabula
import PyPDF2
import spacy
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

"""
General grading comments:
- Generally you shouldn't wrap return statements in parentheses - it works, but it makes them look like function calls
- You put a ton of work into this and it shows.  Good job.
"""

nlp = spacy.load("en_core_web_sm")
path = r'C:\Users\bethk\Desktop\Data_Skills_II\BarStatistics'
os.chdir(path)
state_names = [state.name for state in us.states.STATES] # List comprehension adapted from https://stackoverflow.com/questions/47168881/how-to-get-a-list-of-us-state-names-from-us-1-0-0-package

headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36'}

'''
Bar pass rate dataframe
'''

pass_rate_dict = {2015:'https://thebarexaminer.org/statistics/2015-statistics/persons-taking-and-passing-the-2015-bar-examination/',
                  2016:'https://thebarexaminer.org/statistics/2016-statistics/persons-taking-and-passing-the-2016-bar-examination/',
                  2017:'https://thebarexaminer.org/statistics/2017-statistics/persons-taking-and-passing-the-2017-bar-examination/',
                  2018:'https://thebarexaminer.org/statistics/2018-statistics/persons-taking-and-passing-the-2018-bar-examination/'
}

# Create functions to make a dataframe from the tables on the NCBE website
# (approach derived from Jeff Levy's Spring 2020 Week 8 Lecture 2)

def make_table(url): # Retrieves a table from NCBE bar pass rate website
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'lxml')
    table = soup.find('table')
    return(table)

def create_list(a_table): # Makes a list from the table
    unparsed_rows = []
    for row in a_table.find_all('tr'):
        td_tags = row.find_all('td')
        unparsed_rows.append([val.text for val in td_tags])
# Remove first empty list, remove commas from integers, remove non-letters from Jurisdiction
    del unparsed_rows[0]
    for a_row in unparsed_rows:
        for a_num in [3,4]:
            a_row[a_num] = a_row[a_num].replace(',','')
            try: #JL: good exception capturing here
                int(a_row[a_num]) #JL: you can also use isinstance(a_row[a_num], int) to test datatypes
            except ValueError:
                a_row[a_num] = 0
    for a_row in unparsed_rows:
        a_row[1] = a_row[1].replace('\t\t\tâ€ ', '')
        a_row[1] = a_row[1].replace('\t\t\t*', '')
        a_row[1] = a_row[1].rstrip()
    return(unparsed_rows)

def build_df(a_list): # Makes a dataframe from the list
    the_df = pd.DataFrame(a_list, columns = ['Number', 'Jurisdiction', 'Month', 'Takers', 'Passers', 'Pass_Rate'])
    del the_df['Number'] #JL: can also be done with the drop method
    the_df = the_df.astype(dtype={'Takers':int, 'Passers':int})
    the_df['Pass_Rate'] = the_df['Passers']/the_df['Takers']
    the_df = the_df[the_df['Month'] == 'Total February and July']
    del the_df['Month']
    return(the_df)

def passrates_df(year): # Retrieves or builds the pass rate dataframe
    if os.path.isfile(os.getcwd()+f'\\pass_rates_{year}.csv') == True:
        pass_rate_df = pd.read_csv(os.getcwd()+f'\\pass_rates_{year}.csv')
    else:
        pass_rate_df = build_df(create_list(make_table(pass_rate_dict[year])))
        pass_rate_df.to_csv(f'pass_rates_{year}.csv')
    try:
        del pass_rate_df['Unnamed: 0']
    except KeyError:
        pass
    return(pass_rate_df)

'''
Poverty rates dataframe
'''

def povrates_df(year):
# Retrieve the CSV
    if os.path.isfile(os.getcwd()+'\pov_rates.csv') == True:
        pass
    else:
        pov_rates = pd.read_excel('https://www2.census.gov/programs-surveys/cps/tables/time-series/historical-poverty-people/hstpov21.xlsx', header=4)
        pov_rates.to_csv('pov_rates.csv')
    pov_rates = pd.read_csv('pov_rates.csv')[['STATE', 'Percent']]
# Divide up the poverty rate dataframe by year
    # Get Alabama 2015-2018 indices, removing the duplicate 2017 portion
    ALs = list((pov_rates[pov_rates['STATE']=='Alabama'].index.values))[1:6]
    del ALs[1]
    # Match Alabama indices to years in a dictionary
    yrs = list(pass_rate_dict.keys())
    ALs_yrs_dict = {yrs[i]: ALs[i] for i in range(len(yrs))}
    # Isolate out the right year's data and change column names to match the bar dataframe
    state_rate = []
    for a_num in range(ALs_yrs_dict[year], ALs_yrs_dict[year] + 51):
        state_rate.append([pov_rates['STATE'][a_num], float(pov_rates['Percent'][a_num])/100])
    pov_rates = pd.DataFrame(state_rate, columns=['Jurisdiction', 'Poverty_Rate'])
    return(pov_rates)

'''
Lawyer discipline dataframes
'''

# Get initial dataframes from PDFs (2015 and 2016 need to be broken up by page, partially cleaned, then recombined)

discipline_dict_1 = {2015:'https://www.americanbar.org/content/dam/aba/administrative/professional_responsibility/2015_sold_results.pdf',
                  2016:'https://www.americanbar.org/content/dam/aba/administrative/professional_responsibility/2016sold_results.pdf'}
discipline_dict_2 = {2017:'https://www.americanbar.org/content/dam/aba/administrative/professional_responsibility/2017sold-results.pdf',
                  2018:'https://www.americanbar.org/content/dam/aba/administrative/professional_responsibility/2018sold-results.pdf'
}

def dis_15_16_csvs():
    for year in discipline_dict_1.keys():
        if os.path.isfile(os.getcwd()+f'\discipline_{year}.csv') == True:
            pass
        else:
            tabula.convert_into(discipline_dict_1[year],
                                f'discipline_{year}.csv',
                                output_format = 'csv')
            tabula.convert_into(discipline_dict_1[year],
                                f'discipline_{year}_a.csv',
                                output_format='csv', pages='5')
            tabula.convert_into(discipline_dict_1[year],
                                f'discipline_{year}_b.csv',
                                output_format='csv', pages='6')
            tabula.convert_into(discipline_dict_1[year],
                                f'discipline_{year}_c.csv',
                                output_format='csv', pages='7')

def df_15a():
    dis15a = pd.read_csv('discipline_2015_a.csv', encoding='cp1252', header=6)
    dis15a.columns = ['garbage', 'Jurisdiction', 'junk', 'Num_Lawyers', 'Num_Complaints', 'Central_Intake', 'Separate']
    del dis15a['garbage']
    del dis15a['junk']
    dis15a['Jurisdiction'][13] = 'Iowa'
    dis15a['Jurisdiction'][2] = 'Arizona'
    return(dis15a)

def df_15b():
    dis15b = pd.read_csv('discipline_2015_b.csv', encoding='cp1252', header=6)
    dis15b.columns = ['garbage', 'Jurisdiction', 'junk', 'Num_Lawyers', 'Num_Complaints', 'Central_Intake', 'Separate']
    dis15b['Jurisdiction'][0] = 'Mississippi'
    dis15b['Jurisdiction'][1] = 'to be deleted'
    dis15b['Jurisdiction'][15] = 'New York: 3rd Department'
    dis15b['Num_Lawyers'][15] = '65,000'
    dis15b['Num_Complaints'][15] = '2,098'
    dis15b['Jurisdiction'][3] = 'Montana'
    dis15b['Jurisdiction'][23] = 'Oregon'
# Remove outlier in Complaints_Per_Lawyer: NH 2015
    dis15b = dis15b[dis15b['Jurisdiction'] != 'New Hampshire']
    del dis15b['garbage']
    del dis15b['junk']
    return(dis15b)

def df_15c():
    dis15c = pd.read_csv('discipline_2015_c.csv', encoding='cp1252', header=4)
    dis15c.columns=['Jurisdiction', 'Num_Lawyers', 'Num_Complaints', 'Central_Intake', 'Separate']
    return(dis15c)

def df_16a():
    dis16a = pd.read_csv('discipline_2016_a.csv', encoding='cp1252', header=4)
    del dis16a['Unnamed: 0']
    del dis16a['Unnamed: 1']
    del dis16a['Unnamed: 3']
    dis16a.columns=['Jurisdiction', 'Num_Lawyers', 'Num_Complaints', 'Central_Intake', 'Separate']
#    dis16a['Jurisdiction'][9] = 'Hawaii'
    return(dis16a)

def df_16b():
    dis16b = pd.read_csv('discipline_2016_b.csv', encoding='cp1252', header=4)
    dis16b.columns=['garbage', 'trash', 'Jurisdiction', 'junk', 'Num_Lawyers', 'Num_Complaints', 'Central_Intake', 'Separate']
    dis16b['Jurisdiction'][13] = 'New York: 3rd Department'
    dis16b['Num_Lawyers'][13] = '70,000'
    dis16b['Num_Complaints'][13] = '1,239'
    del dis16b['garbage']
    del dis16b['trash']
    del dis16b['junk']
    return(dis16b)

def df_16c():
    dis16c = pd.read_csv('discipline_2016_c.csv', encoding='cp1252', header=4)
    dis16c.columns=['Jurisdiction', 'Num_Lawyers', 'Num_Complaints', 'Central_Intake', 'Separate']
    return(dis16c)

def consolidate_dis_15_16(year):
    dis_15_16_csvs()
    if year == 2015:
        a = df_15a() #JL: use more descriptive variable names
        b = df_15b()
        c = df_15c()
    elif year == 2016:
        a = df_16a()
        b = df_16b()
        c = df_16c() #JL: when your code takes on this copy-paste structure, that usually means you can probably do it more efficiently with loops, containers, and/or functions
    dis_df = a.append(b).append(c).reset_index(drop=True)
    return(dis_df)

def dis_17_18_csvs():
    for year in discipline_dict_2.keys():
        if os.path.isfile(os.getcwd()+f'\discipline_{year}.csv') == True:
            pass
        else:
            tabula.convert_into(discipline_dict_2[year],
                                f'discipline_{year}.csv',
                                output_format='csv', pages='5-7')

def adjust_columns(dis_df):
    dis_df.columns=['Jurisdiction', 'Num_Lawyers', 'Num_Complaints', 'Central_Intake', 'Separate']
    del dis_df['Central_Intake']
    del dis_df['Separate']

# Rename New York rows, since divided into districts
def rename_ny(dis_df):
# Approach to finding first instance of "New York" derived from https://www.geeksforgeeks.org/python-pandas-series-str-find/
    first_ny_list = dis_df['Jurisdiction'].str.find('New York')
    first_ny = (first_ny_list.values == 0).argmax()
    last_ny_list = dis_df['Jurisdiction'].str.find('North Carolina')
    last_ny = (last_ny_list.values == 0).argmax()
    for num in range(first_ny, last_ny):
        dis_df.loc[num][0] = 'New York'

def remove_non_states(dis_df):
    dis_df['Jurisdiction'] = dis_df['Jurisdiction'].replace("'","", regex=True)
    dis_df = dis_df[dis_df['Jurisdiction'].isin(
state_names)].reset_index(drop=True)
    return(dis_df)

def numeric_columns(dis_df):
    for col in dis_df:
        if col != 'Jurisdiction':
            dis_df[col] = dis_df[col].replace([',', 'N', '\\*'],'', regex=True)
            for num in range(0, len(dis_df)):
                try:
                    int(dis_df[col][num])
                except ValueError:
                    dis_df[col][num] = 0
    dis_df = dis_df.astype(dtype={'Num_Lawyers':int, 'Num_Complaints':int})
    return(dis_df)

# Remove rows with only 0s, consolidate New York rows, and add column with complaints per lawyer
def finalize(dis_df):
    dis_df = dis_df[(dis_df['Num_Lawyers'] != 0) & (dis_df['Num_Complaints'] != 0)]
    ny_lawyers = dis_df.loc[dis_df['Jurisdiction'] == 'New York'].sum()[1]
    ny_complaints = dis_df.loc[dis_df['Jurisdiction'] == 'New York'].sum()[2]
    ny = pd.DataFrame([['New York', ny_lawyers, ny_complaints]], columns=['Jurisdiction', 'Num_Lawyers', 'Num_Complaints'])
    dis_df = dis_df[dis_df['Jurisdiction'] != 'New York']
    dis_df = dis_df.append(ny).sort_values('Jurisdiction').reset_index(drop=True)
    dis_df['Complaints_Per_Lawyer'] = dis_df['Num_Complaints']/dis_df['Num_Lawyers']
    return(dis_df)

def make_disc_dict():
    dis15 = consolidate_dis_15_16(2015)
    dis16 = consolidate_dis_15_16(2016)
    dis_17_18_csvs()
    dis17 = pd.read_csv('discipline_2017.csv', encoding='cp1252', header=3)
    dis18 = pd.read_csv('discipline_2018.csv', encoding='cp1252')
    dis_dfs_dict = {2015:dis15, 2016:dis16, 2017:dis17, 2018:dis18}
    return(dis_dfs_dict)

def get_disc(year):
    a = make_disc_dict()[year]
    adjust_columns(a)
    rename_ny(a)
    a = remove_non_states(a)
    a = numeric_columns(a)
    a = finalize(a)
    return(a)

'''
Unemployment rates dataframe
'''

def unemprates_df():
# Get the data
    if os.path.isfile(os.getcwd()+ r'\unemp_rates.csv') == True:
        pass
    else:
        unemp_rates = pd.read_excel('https://www.ers.usda.gov/webdocs/DataFiles/48747/Unemployment.xls?v=4751.8', header=7)
        unemp_rates.to_csv('unemp_rates.csv')
    unemp = pd.read_csv('unemp_rates.csv')[['area_name', 'Unemployment_rate_2015', 'Unemployment_rate_2016', 'Unemployment_rate_2017', 'Unemployment_rate_2018']]
# Adjust the data
    unemp = unemp[unemp['area_name'].isin(state_names)].reset_index(drop=True)
    unemp = unemp.rename(columns={'area_name': 'Jurisdiction'})
    return(unemp)

'''
Educational attainment dataframe
'''

def edrates_df():
    if os.path.isfile(os.getcwd()+ r'\ed_attainment.csv') == True:
        pass
    else:
        ed = pd.read_excel('https://www.ers.usda.gov/webdocs/DataFiles/48747/Education.xls?v=4751.8', header=4)
        ed.to_csv('ed_attainment.csv')
    ed = pd.read_csv('ed_attainment.csv')
# Remove sub-state data
    ed = ed[ed['Area name'].isin(state_names)].reset_index(drop=True)
# Remove duplicate Washington, D.C.
    ed = ed.drop_duplicates(subset='Area name', inplace=False)
# Restrict columns to percentages, simplify column names, make percentages decimals
    ed = ed[['Area name',
       'Percent of adults with less than a high school diploma, 2014-18',
       'Percent of adults with a high school diploma only, 2014-18',
       "Percent of adults completing some college or associate's degree, 2014-18",
       "Percent of adults with a bachelor's degree or higher, 2014-18"]]
    ed.columns = ['Jurisdiction', 'No_HS', 'HS', 'Some_Coll', 'BA']
    for col in ed.columns:
        if ed[col].dtype == 'float64':
            ed[col] = ed[col]/100
    return(ed)

'''
Race dataframes
'''

# Get race names by key from census PDF
def get_statement(url):
    if ('sc-est2019-alldata6.pdf' in os.getcwd()):
        pass
    else:
        fname = url.split('/')[-1]
        response = requests.get(url)
        with open(os.path.join(os.getcwd(), fname), 'wb') as ofile:
            ofile.write(response.content)

def parse_pdf(a_pdf):
    a_pdf = os.getcwd() + '\\' + a_pdf
    fname = a_pdf.split('/')[-1]
    with open(os.path.join(os.getcwd(), fname), 'rb') as ifile:
        pdf_reader = PyPDF2.PdfFileReader(ifile)
        pages = [pdf_reader.getPage(p) for p in range(1,2)]
        pages = [p.extractText() for p in pages]
    return pages

get_statement('https://www2.census.gov/programs-surveys/popest/technical-documentation/file-layouts/2010-2019/sc-est2019-alldata6.pdf')
codes = str(parse_pdf('sc-est2019-alldata6.pdf')).split('\\n')

racewords = ['White', 'Black', 'Indian', 'Asian', 'Hawaiian', 'races']
hispwords = ['Hispanic']

def get_races(race_set):
    racecodes = []
    for item in codes:
        for race in race_set:
            if (race in item) & ('=' in item):
                racecodes.append(item)
    return(racecodes)

def strip_whitespace(race_set):
    stripped = [item.strip() for item in race_set]
    return(stripped)

def make_race_dict(race_set):
    keys = []
    vals = []
    race_list = strip_whitespace(get_races(race_set))
    for item in race_list:
        first = int(item.split('=')[0].strip())
        second = item.split('=')[1].strip()
        keys.append(first)
        vals.append(second)
    output_dict = dict(zip(keys, vals))
    return(output_dict)

hisp = make_race_dict(hispwords)
nonhisp = make_race_dict(racewords)
nonhisp[1] = 'White'
nonhisp[2] = 'Black'
nonhisp[3] = 'Native'
nonhisp[4] = 'Asian'
nonhisp[5] = 'Pacific'
nonhisp[6] = 'Multiracial'

def race_df(): # Imports race data by state
    if os.path.isfile(os.getcwd()+f'\\race.csv') == True:
        racedata = pd.read_csv(os.getcwd()+f'\\race.csv')
    else:
        racedata = pd.read_csv('https://www2.census.gov/programs-surveys/popest/tables/2010-2019/state/asrh/sc-est2019-alldata6.csv')
        racedata.to_csv('race.csv')
# Remove region codes and aggregates
    racedata = racedata[racedata['ORIGIN'] != 0]
    racedata = racedata[racedata['SEX'] != 0]
    racedata = racedata[['NAME', 'ORIGIN', 'RACE', 'POPESTIMATE2015', 'POPESTIMATE2016', 'POPESTIMATE2017', 'POPESTIMATE2018']]
    return(racedata)

racedata = race_df()

# Map in the race names
racedata['Hisp'] = racedata['ORIGIN'].map(hisp)
racedata['Nonhisp'] = racedata['RACE'].map(nonhisp)
racedata['Race'] = racedata[['Nonhisp','Hisp']].apply(lambda x: ', '.join(x), axis=1)
racedata = racedata[['NAME', 'POPESTIMATE2015', 'POPESTIMATE2016',
       'POPESTIMATE2017', 'POPESTIMATE2018', 'Race']]

# Make a dataframe with racial population estimates for each year
def race_by_state(year):
    r = racedata[['NAME', f'POPESTIMATE{year}', 'Race']]
    r = r.groupby(['NAME', 'Race']).sum().reset_index()
    r = r.pivot(index='NAME', columns='Race', values=f'POPESTIMATE{year}').reset_index()
# Get race numbers as percentages then rename state column to match other dataframes
    r['Total'] = r.sum(axis=1)
    for col in r.columns[1:]:
        r[col] = r[col]/r['Total']
    del r['Total']
    r = r.rename(columns={'NAME':'Jurisdiction'})
    return(r)

'''
Merge dataframes
'''

def merge_it(year): # Merges dataframes for a given year
# Initial merge of all dataframes except unemployment
    merged = pd.merge(passrates_df(year), povrates_df(year))
    merged = pd.merge(merged, get_disc(year))
    merged = pd.merge(merged, edrates_df())
    merged = pd.merge(merged, race_by_state(year))
# Add columns for unemployment rate and year
    unemp = unemprates_df()
    unemp = unemp[unemp['Jurisdiction'].isin(
merged['Jurisdiction'])].reset_index(drop=True)
    merged['Unemployment_Rate'] = unemp[f'Unemployment_rate_{year}']/100
    merged['Year'] = year
    return(merged)
# Note: the merge will drop US territories (which are in the bar passage rates dataframe), leaving only states and Washington, D.C.

def final_export(final_df):
     if os.path.isfile(os.getcwd()+'final_df.csv') == True:
        pass
     else:
        final_df.to_csv('final_df.csv')

def merge_dfs(start, end): # Combines all years into one dataframe
    dfs = []
    for year in range(start, end+1):
        merged = merge_it(year)
        dfs.append(merged)
    all = pd.concat(dfs, axis=0).reset_index(drop=True)
# Rename columns to prepare for plotting
    all = all.rename(columns={'BA':'Percent with BA'})
    all.columns = all.columns.str.replace('_',' ')
    final_export(all)
    return(all)

# Make dataframe with averages for all years, 2015-2018
alldata = merge_dfs(2015, 2018)
print(alldata)

'''
Regressions
'''

def regress(regressor, regressand):
    x = regressor
    y = regressand
    x = sm.add_constant(x)
    model = sm.OLS(y, x).fit()
    print(model.summary())

kitchen_sink_data_1 = alldata[['Takers', 'Pass Rate', 'Poverty Rate',
       'Num Lawyers', 'No HS', 'HS', 'Percent with BA', 'Asian, Hispanic',
       'Asian, Not Hispanic',
       'Black, Hispanic', 'Black, Not Hispanic', 'Multiracial, Hispanic',
       'Multiracial, Not Hispanic', 'Native, Hispanic', 'Native, Not Hispanic',
       'Pacific, Hispanic', 'Pacific, Not Hispanic', 'White, Hispanic',
       'White, Not Hispanic', 'Unemployment Rate']]
kitchen_sink_data_2 = alldata[['Takers', 'Poverty Rate',
       'Num Lawyers', 'No HS', 'HS', 'Percent with BA', 'Asian, Hispanic', 'Asian, Not Hispanic',
       'Black, Hispanic', 'Black, Not Hispanic', 'Multiracial, Hispanic',
       'Multiracial, Not Hispanic', 'Native, Hispanic', 'Native, Not Hispanic',
       'Pacific, Hispanic', 'Pacific, Not Hispanic', 'White, Hispanic',
       'White, Not Hispanic', 'Unemployment Rate']]

# Complaints per lawyer regressions
regress(kitchen_sink_data_1, alldata['Complaints Per Lawyer'])
regress(alldata['Pass Rate'], alldata['Complaints Per Lawyer'])

# Pass rate regressions
regress(kitchen_sink_data_2, alldata['Pass Rate'])
regress(alldata['Black, Not Hispanic'], alldata['Pass Rate'])
regress(alldata['White, Not Hispanic'], alldata['Pass Rate'])
regress(alldata[['White, Not Hispanic', 'Poverty Rate']], alldata['Pass Rate'])
regress(alldata[['White, Not Hispanic', 'Unemployment Rate']], alldata['Pass Rate'])
regress(alldata['Percent with BA'], alldata['Pass Rate'])

'''
Plotting
'''

def scatter(x,y, color, a_df):
    plt.figure() # Prevents piling plots on top of one another
    sns.set_style("darkgrid")
    ax = sns.scatterplot(x,y, data=a_df, hue=color, s=50)
    ax.set(xlabel=x, ylabel=y, title=f'{x} Plotted Against {y}')
    yvals = ax.get_yticks() # Change ticks to percentages: solution at https://stackoverflow.com/questions/31357611/format-y-axis-as-percent
    ax.set_yticklabels(['{:,.0%}'.format(y) for y in yvals])
    xvals = ax.get_xticks()
    ax.set_xticklabels(['{:,.0%}'.format(x) for x in xvals])
    plt.savefig(f'{x} vs. {y}.png')

def reg_plot(x,y, a_df):
    plt.figure()
    sns.set_style("darkgrid")
    ax = sns.regplot(x,y, data=a_df)
    ax.set(xlabel=x, ylabel=y, title=f'{x} Plotted Against {y} with Line of Best Fit')
    yvals = ax.get_yticks()
    ax.set_yticklabels(['{:,.0%}'.format(y) for y in yvals])
    xvals = ax.get_xticks()
    ax.set_xticklabels(['{:,.0%}'.format(x) for x in xvals])
    plt.savefig(f'{x} vs. {y} regression.png')

# Examine effect of poverty on passage rates
scatter('Poverty Rate', 'Pass Rate', 'Percent with BA', alldata)
reg_plot('Poverty Rate', 'Pass Rate', alldata)

# Examine effect of unemployment on passage rates
scatter('Unemployment Rate', 'Pass Rate', 'Poverty Rate', alldata)
reg_plot('Unemployment Rate', 'Pass Rate', alldata)

# Examine effect of race on passage rates
scatter('White, Not Hispanic', 'Pass Rate', 'Poverty Rate', alldata)
reg_plot('White, Not Hispanic', 'Pass Rate', alldata)

# Examine effect of passage rates on complaints per lawyer
scatter('Pass Rate', 'Complaints Per Lawyer', 'Poverty Rate', alldata)
reg_plot('Pass Rate', 'Complaints Per Lawyer', alldata)
